{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 --conf spark.cassandra.connection.host=172.20.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/06 06:59:32 WARN Utils: Your hostname, Obi-Wan resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface enp6s0)\n",
      "23/08/06 06:59:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/narvy/Avatarify/miniconda3/envs/data-infra/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/narvy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/narvy/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fa99b634-9c1b-4f1d-a534-863496b30775;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/narvy/Avatarify/miniconda3/envs/data-infra/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 275ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fa99b634-9c1b-4f1d-a534-863496b30775\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/6ms)\n",
      "23/08/06 06:59:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/06 06:59:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, ArrayType\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.master('local[12]').appName(\"btc_insertion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = StructType([\n",
    "        StructField(\"ot\", ArrayType(FloatType())),\n",
    "        StructField(\"ct\", ArrayType(FloatType())),\n",
    "        StructField(\"o\", ArrayType(FloatType())),\n",
    "        StructField(\"h\", ArrayType(FloatType())),\n",
    "        StructField(\"l\", ArrayType(FloatType())),\n",
    "        StructField(\"c\", ArrayType(FloatType())),\n",
    "        StructField(\"v\", ArrayType(FloatType()))\n",
    "])\n",
    "\n",
    "data = json.load(open('../data/dataset.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/06 06:59:44 WARN TaskSetManager: Stage 0 contains a task of very large size (188491 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:====================================================>    (11 + 1) / 12]\r"
     ]
    }
   ],
   "source": [
    "d_frames = {}\n",
    "\n",
    "conf = SparkConf().setAppName(\"df_cassandra\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "for interval in data:\n",
    "    data_i = data[interval]\n",
    "    df = spark.createDataFrame([Row(\n",
    "        ot = data_i['ot'],\n",
    "        ct = data_i['ct'],\n",
    "        o = data_i['o'],\n",
    "        h = data_i['h'],\n",
    "        l = data_i['l'],\n",
    "        c = data_i['c'],\n",
    "        v = data_i['v']\n",
    "    )], schema=input_schema)\n",
    "    \n",
    "    df = df\\\n",
    "        .withColumn('ot', explode(df['ot']))\\\n",
    "        .withColumn('ct', explode(df['ct']))\\\n",
    "        .withColumn('o', explode(df['o']))\\\n",
    "        .withColumn('h', explode(df['h']))\\\n",
    "        .withColumn('l', explode(df['l']))\\\n",
    "        .withColumn('c', explode(df['c']))\\\n",
    "        .withColumn('v', explode(df['v']))\n",
    "    \n",
    "    d_frames[interval] = df = df.withColumn('ot', col('ot').cast(\"Timestamp\")).withColumn('ct', col('ct').cast(\"Timestamp\"))\n",
    "    \n",
    "    partitions = 100\n",
    "    p_len = df.count() // n_splits\n",
    "    \n",
    "    for s in range(partitions):\n",
    "        parted_df = df.limit(p_len)\n",
    "        df = df.subtract(parted_df)\n",
    "    \n",
    "        parted_df.write\\\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "            .options(table = f'raw_{interval}', keyspace = 'bitcoin_chart')\\\n",
    "            .mode('append')\\\n",
    "            .save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of splits you want\n",
    "n_splits = 4\n",
    "  \n",
    "# Calculate count of each dataframe rows\n",
    "each_len = prod_df.count() // n_splits\n",
    "  \n",
    "# Create a copy of original dataframe\n",
    "copy_df = prod_df\n",
    "  \n",
    "# Iterate for each dataframe\n",
    "i = 0\n",
    "while i < n_splits:\n",
    "  \n",
    "    # Get the top `each_len` number of rows\n",
    "    temp_df = copy_df.limit(each_len)\n",
    "  \n",
    "    # Truncate the `copy_df` to remove\n",
    "    # the contents fetched for `temp_df`\n",
    "    copy_df = copy_df.subtract(temp_df)\n",
    "  \n",
    "    # View the dataframe\n",
    "    temp_df.show(truncate=False)\n",
    "  \n",
    "    # Increment the split number\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

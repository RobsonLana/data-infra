{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 --conf spark.cassandra.connection.host=192.168.112.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, ArrayType\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.master('local[12]').appName(\"btc_insertion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = StructType([\n",
    "        StructField(\"ot\", ArrayType(FloatType())),\n",
    "        StructField(\"ct\", ArrayType(FloatType())),\n",
    "        StructField(\"o\", ArrayType(FloatType())),\n",
    "        StructField(\"h\", ArrayType(FloatType())),\n",
    "        StructField(\"l\", ArrayType(FloatType())),\n",
    "        StructField(\"c\", ArrayType(FloatType())),\n",
    "        StructField(\"v\", ArrayType(FloatType()))\n",
    "])\n",
    "\n",
    "data = json.load(open('../data/dataset.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"df_cassandra\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "batch_size = 500\n",
    "current_index = 0\n",
    "\n",
    "for interval in data:\n",
    "    data_i = data[interval]\n",
    "\n",
    "    while len(data_i['ot'][current_index:batch_size]) > 0:\n",
    "    \n",
    "        df = spark.createDataFrame([Row(\n",
    "            ot = data_i['ot'][current_index:batch_size],\n",
    "            ct = data_i['ct'][current_index:batch_size],\n",
    "            o = data_i['o'][current_index:batch_size],\n",
    "            h = data_i['h'][current_index:batch_size],\n",
    "            l = data_i['l'][current_index:batch_size],\n",
    "            c = data_i['c'][current_index:batch_size],\n",
    "            v = data_i['v'][current_index:batch_size]\n",
    "        )], schema=input_schema)\n",
    "\n",
    "        current_index = data_i['ot'].index(data_i['ot'][current_index:batch_size][-1])\n",
    "    \n",
    "        df = df\\\n",
    "            .withColumn('ot', explode(df['ot']))\\\n",
    "            .withColumn('ct', explode(df['ct']))\\\n",
    "            .withColumn('o', explode(df['o']))\\\n",
    "            .withColumn('h', explode(df['h']))\\\n",
    "            .withColumn('l', explode(df['l']))\\\n",
    "            .withColumn('c', explode(df['c']))\\\n",
    "            .withColumn('v', explode(df['v']))\n",
    "    \n",
    "        df = df.withColumn('ot', col('ot').cast(\"Timestamp\")).withColumn('ct', col('ct').cast(\"Timestamp\"))\n",
    "    \n",
    "        df.write\\\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "            .options(table = f'raw_{interval}', keyspace = 'bitcoin_chart')\\\n",
    "            .mode('append')\\\n",
    "            .save()\n",
    "        \n",
    "        del df\n",
    "        del data_i\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
